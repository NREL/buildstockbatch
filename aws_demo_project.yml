schema_version: '0.3'
buildstock_directory: ../resstock  # Relative to this file or absolute
project_directory: project_national  # Relative to buildstock_directory
output_directory: ../demo_test_outputs
weather_files_url: https://data.nrel.gov/system/files/156/BuildStock_TMY3_FIPS.zip

baseline:
  n_buildings_represented: 133172057  # Total number of residential dwelling units in contiguous United States, including unoccupied units, resulting from acensus tract level query of ACS 5-yr 2016 (i.e. 2012-2016), using this script: https://github.com/NREL/resstock-estimation/blob/master/sources/spatial/tsv_maker.py.

sampler:
  type: residential_quota # change to residential_quota_downselect to do downselect
  args:
    n_datapoints: 4
    # logic:
    #   - Geometry Building Type RECS|Single-Family Detached
    #   - Vacancy Status|Occupied
    # resample: false # Uncomment and specify logic you you want to downselect to a subset of the building stock

workflow_generator:
  type: residential_default  
  args:
    timeseries_csv_export:
      reporting_frequency: Hourly
      include_enduse_subcategories: true

upgrades:
  - upgrade_name: Triple-Pane Windows
    options:
      - option: Windows|Low-E, Triple, Non-metal, Air, L-Gain
        costs:
          - value: 45.77
            multiplier: Window Area (ft^2)
        lifetime: 30

aws:
  # The job_identifier must be unique, start with alpha, not include dashes, and limited to 10 chars
  job_identifier: ext_demo
  s3:
    bucket: resbldg-datasets
    prefix: testing/external_demo_project
  emr:
    slave_instance_count: 1
  region: us-west-2
  use_spot: true
  batch_array_size: 4
  # To receive email updates on job progress accept the request to receive emails that will be sent from Amazon
  notifications_email: user@example.com

postprocessing:
  aws:
    region_name: 'us-west-2'
    s3:
      bucket: resbldg-datasets
      prefix: resstock-athena/external_demo
    athena:
      glue_service_role: service-role/AWSGlueServiceRole-default
      database_name: ext_demo1
      max_crawling_time: 300 #time to wait for the crawler to complete before aborting it
